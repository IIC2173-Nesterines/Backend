services: # The containers we are going to run
  api:
    build:
      context: ./API # The path in the machine that have the files, in this case root
      dockerfile: Dockerfile # The name of the Dockerfile definition
    container_name: app
    restart: unless-stopped
    volumes:
      - ./API/src:/api/src # To copy changes in the local machine to the container.
    ports:
      - "${API_PORT}:${API_PORT}" # To expose the app to the machine localhost (machine-port:container:port)
    networks:
      - api-network # Connect the app to the network, in this case to use the database
    depends_on: # Build dependencies in case one containers needs another to be builded first
      - db # To build after the database
    environment:
      DATABASE_USER: ${DATABASE_USER}
      DATABASE_PASSWORD: ${DATABASE_PASSWORD}
  broker:
    build:
      context: ./Broker # The path in the machine that have the files, in this case root
      dockerfile: Dockerfile # The name of the Dockerfile definition
    container_name: broker
    restart: unless-stopped
    volumes:
      - ./Broker:/broker # To copy changes in the local machine to the container.
    networks:
      - api-network # Connect the app to the network, in this case to use the database
    depends_on: # Build dependencies in case one containers needs another to be builded first
      - db # To build after the database
  db: # The database service
    image: postgres # Using the oficial dockerhub image
    container_name: postgres
    networks:
      - api-network # Connect to the network to be used by other containers (ej: the web server)
    environment: # all the enviroment variables defined in the container (see postgres official image docs https://hub.docker.com/_/postgres)
      POSTGRES_USER: ${DATABASE_USER}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}
      POSTGRES_DB: ${DATABASE_NAME}
    
    volumes:
      # To persist the data in the database we have a volume that persist after container deletion
      # and copy the files from the container to the volume.
      - db-volume:/var/lib/postgresql/data 
  redis-broker:
    image: redis:7
    volumes:
      - ./Workers/redis/redis.conf:/usr/local/etc/redis/redis.conf
    networks:
      - api-network
  producer:
    build:
      context: ./Workers/project/
      dockerfile: Dockerfile
    command: uvicorn producer:app --reload --host 0.0.0.0 --port ${REDIS_PORT}
    volumes:
      - ./Workers/project:/opt/
    expose:
      - ${REDIS_PORT}
    networks:
      - api-network
    ports:
      - "${REDIS_PORT}:${REDIS_PORT}"
    environment:
      CELERY_BROKER_URL: ${CELERY_BROKER_URL}
      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND}
    depends_on:
      - redis-broker
  consumer:
    build:
      context: ./Workers/project
      dockerfile: Dockerfile
    deploy:
      replicas: 2
    command: celery -A consumer.celery_app worker --loglevel=INFO --purge --concurrency=1
    networks:
      - api-network
    volumes:
      - ./Workers/project:/opt/
    depends_on:
      - producer
    environment:
      CELERY_BROKER_URL: ${CELERY_BROKER_URL}
      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND}
  worker-dashboard:
    build:
      context: ./Workers/project
      dockerfile: Dockerfile
    command: bash -c "sleep 10; celery -A consumer.celery_app flower --loglevel=INFO --url_prefix=dashboard;"
    expose:
      - ${FLOWER_PORT}
    ports:
      - '${FLOWER_PORT}:${FLOWER_PORT}'
    volumes:
      - ./Workers/project:/opt/
    depends_on:
      - consumer
    networks:
      - api-network
    environment:
      CELERY_BROKER_URL: ${CELERY_BROKER_URL}
      CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND}

networks: # All the networks that connects our containers
  api-network:
    driver: bridge

volumes: # All the named volumes needed to persist data after container deletion
  db-volume: